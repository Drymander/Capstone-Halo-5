{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halo 5 Match Prediction Model\n",
    "\n",
    "Having spent many hours playing the specific Super Fiesta Party playlist, it seemed clear to me that we were being pit against players with a wide range of skill.  Sometimes, we would be beaten so badly that we couldn't help but laugh, despite having played just fine for the past few games.  \n",
    "\n",
    "I wanted to know for certain whether or not games were, in a sense, predetermined in any given direction.  Of course this would never be the intention of a multiplayer matchmaking system.  In theory, a perfect match making system would always be a 50/50 matchup.  Using what we learned in the first section about data from the API, let's see how close to 50/50 matchmaking really is.\n",
    "\n",
    "Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "We'll start by importing the same packages as our EDA notebook a long with an extensive set of sci-kit learn tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:43.836658Z",
     "start_time": "2021-07-26T16:12:42.489355Z"
    }
   },
   "outputs": [],
   "source": [
    "#Standard Packages\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "# Packages used for API calls and data processing\n",
    "import requests\n",
    "import json\n",
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "import ast\n",
    "import time\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "api_key = 'ceeaacb7cf024c7485e00ef8457e42dc'\n",
    "gamertag = 'Drymander'\n",
    "from tqdm import tqdm\n",
    "# !pip install isodate\n",
    "import isodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T00:49:36.750437Z",
     "start_time": "2021-07-30T00:49:36.741427Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/api_keys.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e5e8eb9e641e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mget_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/api_keys.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-e5e8eb9e641e>\u001b[0m in \u001b[0;36mget_keys\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/api_keys.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/api_keys.py'"
     ]
    }
   ],
   "source": [
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "get_keys('/api_keys.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:44.449797Z",
     "start_time": "2021-07-26T16:12:43.837658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing tools\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict,cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,OneHotEncoder\n",
    "scaler = StandardScaler()\n",
    "from sklearn import metrics\n",
    "\n",
    "# Models & Utilities\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "When I first started this project, I tried running models using matches from my personal history.  However, there was one major issue wwith that choice.  In order to get the features for our model, we will need to pull information that is only available from the API in a 'total lifetime' format.  \n",
    "\n",
    "This means that if I played with a player **10 months ago**, I would actually only be able to pull features for that player for their stats **today**.  In other words, only the past ~3 weeks or so of matches could be considered passable as quality data where every player's time line synced with when the match was played.\n",
    "\n",
    "Not satisfied with a model with poor quality and very scattered cross variance scores, I decided to go about it in a different way.\n",
    "\n",
    "From my earlier data collection, I was able to amass a list of unique gamertag names that I have played with throughout time.  We will build a process to pull each of those players' 25 most recent games, put each game into one line of the dataframe, and limit the date range of those matches so that all data will be properly synced.\n",
    "\n",
    "# Functions\n",
    "\n",
    "To pull this off, we'll need to chain together some functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamertag for API\n",
    "\n",
    "This is a simple function to prepare anyone's gamertag from how it would normally appear to how it needs to be formatted for the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:44.456799Z",
     "start_time": "2021-07-26T16:12:44.451798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this+is+a+test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare gamertag for API\n",
    "def gamertag_for_api(gamertag):\n",
    "    \n",
    "    # Replace spaces with '+'\n",
    "    gamertag = gamertag.replace(' ','+')\n",
    "    return gamertag\n",
    "\n",
    "# Testing the function\n",
    "gamertag_for_api('this is a test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Recent Match\n",
    "\n",
    "This pulls the most recent match information for any given player.\n",
    "\n",
    "It uses two API calls.  The first will give us the match ID and date of the most recent game any player has played by specifying their gamertag.  The second calls the match results API, which gives us the gamertags of all players in the match as well as information on winner / loser / tie etc.  It will also give us Spartan Rank and Total XP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:46.007570Z",
     "start_time": "2021-07-26T16:12:44.458799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to pull most recent match stats into JSON format\n",
    "# Uses two separate API calls, one from player history and another from match details\n",
    "def pull_recent_match(how_recent, api_key=api_key, explore=False, gamertag='Drymander'):\n",
    "    \n",
    "    # Use gamertag_for_api function to remove any spaces\n",
    "    gamertag = gamertag_for_api(gamertag)\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Ocp-Apim-Subscription-Key': api_key,\n",
    "    }\n",
    "    # Pulls from arena mode, how_recent is how far to go back in the match history\n",
    "    # 'count' refers to the number of matches to pull\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'modes': 'arena',\n",
    "        'start': how_recent,\n",
    "        'count': 1,\n",
    "        'include-times': True,\n",
    "    })\n",
    "    \n",
    "    # Try this, otherwise return error message\n",
    "    try:\n",
    "        \n",
    "        # Connect to API and pull most recent match for specified gamer\n",
    "        conn = http.client.HTTPSConnection('www.haloapi.com')\n",
    "        conn.request(\"GET\", f\"/stats/h5/players/{gamertag}/matches?%s\" % params, \"{body}\", headers)\n",
    "        response = conn.getresponse()\n",
    "        latest_match = json.loads(response.read())\n",
    "        \n",
    "        # Identify match ID and match date\n",
    "        match_id = latest_match['Results'][0]['Id']['MatchId']\n",
    "        match_date = latest_match['Results'][0]['MatchCompletedDate']['ISO8601Date']\n",
    "        \n",
    "        # Rest for 1.01 seconds to not get blocked by API\n",
    "        time.sleep(1.01)\n",
    "        \n",
    "        # Using match_id, pull details from match\n",
    "        conn.request(\"GET\", f\"/stats/h5/arena/matches/{match_id}?%s\" % params, \"{body}\", headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "        \n",
    "        # Option to return as byte string for alternative viewing\n",
    "        if explore == True:\n",
    "            print(data)\n",
    "        else:\n",
    "            # Append match ID and date from player history API call\n",
    "            match_results = json.loads(data)\n",
    "            match_results['MatchId'] = match_id\n",
    "            match_results['Date'] = match_date\n",
    "        conn.close()\n",
    "    \n",
    "    # Print error if issue with calling API\n",
    "    except Exception as e:\n",
    "        print(f\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    # Return match results as JSON\n",
    "    return match_results\n",
    "\n",
    "# Show result\n",
    "match_results = pull_recent_match(0, explore=False, gamertag='Drymander')\n",
    "# match_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Base Dataframe\n",
    "\n",
    "Now that we have our match results JSON for the most recent match, we'll build a base dataframe similar to what we built in the EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:47.419706Z",
     "start_time": "2021-07-26T16:12:46.008570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>MatchId</th>\n",
       "      <th>GameBaseVariantId</th>\n",
       "      <th>PlaylistId</th>\n",
       "      <th>MapVariantId</th>\n",
       "      <th>DNF</th>\n",
       "      <th>TeamId</th>\n",
       "      <th>PlayerTeam</th>\n",
       "      <th>Winner</th>\n",
       "      <th>TeamColor</th>\n",
       "      <th>Gamertag</th>\n",
       "      <th>SpartanRank</th>\n",
       "      <th>PrevTotalXP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07-29 01:41:02.570</td>\n",
       "      <td>31bfdcf5-3dd4-441d-b260-9a8637d721e6</td>\n",
       "      <td>a2949322-dc84-45ab-8454-cf94fb28c189</td>\n",
       "      <td>f0c9ef9a-48bd-4b24-9db3-2c76b4e23450</td>\n",
       "      <td>e2c25cc8-8f51-44ba-bcde-ff08993b01c8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Enemy</td>\n",
       "      <td>Victory</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Doomnwo</td>\n",
       "      <td>149</td>\n",
       "      <td>23749271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-07-29 01:41:02.570</td>\n",
       "      <td>31bfdcf5-3dd4-441d-b260-9a8637d721e6</td>\n",
       "      <td>a2949322-dc84-45ab-8454-cf94fb28c189</td>\n",
       "      <td>f0c9ef9a-48bd-4b24-9db3-2c76b4e23450</td>\n",
       "      <td>e2c25cc8-8f51-44ba-bcde-ff08993b01c8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Enemy</td>\n",
       "      <td>Victory</td>\n",
       "      <td>Blue</td>\n",
       "      <td>JoelODST117</td>\n",
       "      <td>149</td>\n",
       "      <td>23720290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-07-29 01:41:02.570</td>\n",
       "      <td>31bfdcf5-3dd4-441d-b260-9a8637d721e6</td>\n",
       "      <td>a2949322-dc84-45ab-8454-cf94fb28c189</td>\n",
       "      <td>f0c9ef9a-48bd-4b24-9db3-2c76b4e23450</td>\n",
       "      <td>e2c25cc8-8f51-44ba-bcde-ff08993b01c8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Enemy</td>\n",
       "      <td>Victory</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Mx J3NY Mx</td>\n",
       "      <td>146</td>\n",
       "      <td>9194326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-07-29 01:41:02.570</td>\n",
       "      <td>31bfdcf5-3dd4-441d-b260-9a8637d721e6</td>\n",
       "      <td>a2949322-dc84-45ab-8454-cf94fb28c189</td>\n",
       "      <td>f0c9ef9a-48bd-4b24-9db3-2c76b4e23450</td>\n",
       "      <td>e2c25cc8-8f51-44ba-bcde-ff08993b01c8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Enemy</td>\n",
       "      <td>Victory</td>\n",
       "      <td>Blue</td>\n",
       "      <td>KarryDahZX</td>\n",
       "      <td>149</td>\n",
       "      <td>18940553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-29 01:41:02.570</td>\n",
       "      <td>31bfdcf5-3dd4-441d-b260-9a8637d721e6</td>\n",
       "      <td>a2949322-dc84-45ab-8454-cf94fb28c189</td>\n",
       "      <td>f0c9ef9a-48bd-4b24-9db3-2c76b4e23450</td>\n",
       "      <td>e2c25cc8-8f51-44ba-bcde-ff08993b01c8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Player</td>\n",
       "      <td>Defeat</td>\n",
       "      <td>Red</td>\n",
       "      <td>Drymander</td>\n",
       "      <td>148</td>\n",
       "      <td>15735444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date                               MatchId  \\\n",
       "1 2021-07-29 01:41:02.570  31bfdcf5-3dd4-441d-b260-9a8637d721e6   \n",
       "5 2021-07-29 01:41:02.570  31bfdcf5-3dd4-441d-b260-9a8637d721e6   \n",
       "6 2021-07-29 01:41:02.570  31bfdcf5-3dd4-441d-b260-9a8637d721e6   \n",
       "7 2021-07-29 01:41:02.570  31bfdcf5-3dd4-441d-b260-9a8637d721e6   \n",
       "0 2021-07-29 01:41:02.570  31bfdcf5-3dd4-441d-b260-9a8637d721e6   \n",
       "\n",
       "                      GameBaseVariantId                            PlaylistId  \\\n",
       "1  a2949322-dc84-45ab-8454-cf94fb28c189  f0c9ef9a-48bd-4b24-9db3-2c76b4e23450   \n",
       "5  a2949322-dc84-45ab-8454-cf94fb28c189  f0c9ef9a-48bd-4b24-9db3-2c76b4e23450   \n",
       "6  a2949322-dc84-45ab-8454-cf94fb28c189  f0c9ef9a-48bd-4b24-9db3-2c76b4e23450   \n",
       "7  a2949322-dc84-45ab-8454-cf94fb28c189  f0c9ef9a-48bd-4b24-9db3-2c76b4e23450   \n",
       "0  a2949322-dc84-45ab-8454-cf94fb28c189  f0c9ef9a-48bd-4b24-9db3-2c76b4e23450   \n",
       "\n",
       "                           MapVariantId  DNF  TeamId PlayerTeam   Winner  \\\n",
       "1  e2c25cc8-8f51-44ba-bcde-ff08993b01c8  0.0     1.0      Enemy  Victory   \n",
       "5  e2c25cc8-8f51-44ba-bcde-ff08993b01c8  0.0     1.0      Enemy  Victory   \n",
       "6  e2c25cc8-8f51-44ba-bcde-ff08993b01c8  0.0     1.0      Enemy  Victory   \n",
       "7  e2c25cc8-8f51-44ba-bcde-ff08993b01c8  0.0     1.0      Enemy  Victory   \n",
       "0  e2c25cc8-8f51-44ba-bcde-ff08993b01c8  0.0     0.0     Player   Defeat   \n",
       "\n",
       "  TeamColor     Gamertag SpartanRank PrevTotalXP  \n",
       "1      Blue      Doomnwo         149    23749271  \n",
       "5      Blue  JoelODST117         149    23720290  \n",
       "6      Blue   Mx J3NY Mx         146     9194326  \n",
       "7      Blue   KarryDahZX         149    18940553  \n",
       "0       Red    Drymander         148    15735444  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to build the base dataframe for a single match\n",
    "# Designed to take in the JSON provided by the pull_recent_match function\n",
    "def build_base_dataframe(match_results, gamertag):\n",
    "    \n",
    "    # Build empty base match dataframe\n",
    "    df = pd.DataFrame()\n",
    "    columns = [\n",
    "        'Finished'\n",
    "        'TeamId',\n",
    "        'Gamertag',\n",
    "        'SpartanRank',\n",
    "        'PrevTotalXP',\n",
    "    ]\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    # Populate base match dataframe with player stats for each player\n",
    "    i = 0\n",
    "    for player in match_results['PlayerStats']:\n",
    "\n",
    "        player_dic = {}\n",
    "        # Team ID\n",
    "        player_dic['DNF'] = match_results['PlayerStats'][i]['DNF']\n",
    "        player_dic['TeamId'] = match_results['PlayerStats'][i]['TeamId']\n",
    "        # Team Color\n",
    "        player_dic['TeamColor'] = match_results['PlayerStats'][i]['TeamId']\n",
    "        # Gamer Tag\n",
    "        player_dic['Gamertag'] = match_results['PlayerStats'][i]['Player']['Gamertag']\n",
    "        # Spartan Rank\n",
    "        player_dic['SpartanRank'] = match_results['PlayerStats'][i]['XpInfo']['SpartanRank']\n",
    "        # Previous Total XP\n",
    "        player_dic['PrevTotalXP'] = match_results['PlayerStats'][i]['XpInfo']['PrevTotalXP']\n",
    "        df = df.append(player_dic, ignore_index=True)\n",
    "        i += 1\n",
    "    \n",
    "    ########## DATE, GAME VARIANT, MAP ID, MATCH ID, PLAYLIST ID ##########\n",
    "    df['Date'] = match_results['Date']\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.tz_convert(None)\n",
    "#     df['Date'] = df['Date'].floor('T')\n",
    "    df['MatchId'] = match_results['MatchId']\n",
    "    df['GameBaseVariantId'] = match_results['GameBaseVariantId']\n",
    "    df['MapVariantId'] = match_results['MapVariantId']\n",
    "    df['PlaylistId'] = match_results['PlaylistId']\n",
    "    \n",
    "    ########## DEFINE PLAYER TEAM ##########\n",
    "    playerteam = df.loc[df['Gamertag'] == gamertag, 'TeamId'].values[0]\n",
    "    if playerteam == 0:\n",
    "        enemyteam = 1   \n",
    "    else:\n",
    "        enemyteam = 0\n",
    "        \n",
    "    df['PlayerTeam'] = df['TeamId'].map({playerteam:'Player', enemyteam:'Enemy'})\n",
    "    \n",
    "    if match_results['TeamStats'][0]['TeamId'] == playerteam:\n",
    "        playerteam_stats = match_results['TeamStats'][0]\n",
    "        enemyteam_stats = match_results['TeamStats'][1]\n",
    "    else: \n",
    "        playerteam_stats = match_results['TeamStats'][1]\n",
    "        enemyteam_stats = match_results['TeamStats'][0]\n",
    "    \n",
    "    ########## DETERMINE WINNER ##########\n",
    "    # Tie\n",
    "    if playerteam_stats['Rank'] == 1 and enemyteam_stats['Rank'] == 1:\n",
    "        df['Winner'] = 'Tie'\n",
    "    # Player wins\n",
    "    elif playerteam_stats['Rank'] == 1 and enemyteam_stats['Rank'] == 2:\n",
    "        df['Winner'] = df['TeamId'].map({playerteam:'Victory', enemyteam:'Defeat'})\n",
    "    # Enemy wins\n",
    "    elif playerteam_stats['Rank'] == 2 and enemyteam_stats['Rank'] == 1:\n",
    "        df['Winner'] = df['TeamId'].map({enemyteam:'Victory', playerteam:'Defeat'})\n",
    "    # Error handling\n",
    "    else:\n",
    "        winner = 'Error determining winner'\n",
    "    \n",
    "    ########## TEAM COLOR ##########\n",
    "    df['TeamColor'] = df['TeamId'].map({0:'Red', 1:'Blue'})\n",
    "    \n",
    "    # Set columns\n",
    "    df = df[['Date', 'MatchId', 'GameBaseVariantId', 'PlaylistId', 'MapVariantId', 'DNF',\n",
    "             'TeamId', 'PlayerTeam', 'Winner', 'TeamColor', \n",
    "             'Gamertag', 'SpartanRank', 'PrevTotalXP',\n",
    "            ]]\n",
    "    # Sort match by winning team\n",
    "    df = df.sort_values(by=['Winner'], ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = build_base_dataframe(pull_recent_match(8), 'Drymander')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Player List\n",
    "\n",
    "Now that we have our base dataframe, we'll want to use the gamertags in the match to get more extensive player information.  First, we'll need to prepare their gamertags for the API, similar to how we did it for our first function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:47.427709Z",
     "start_time": "2021-07-26T16:12:47.420706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doomnwo,JoelODST117,Mx+J3NY+Mx,KarryDahZX,Drymander,ToweringAsh97,Andy2542,JohnyUL'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to combine all gamertags from the match and prepare them in string\n",
    "# format for the next API call\n",
    "def get_player_list(df):\n",
    "    \n",
    "    # Create list from our df['Gamertag'] column and remove the brackets\n",
    "    player_list = str(list(df['Gamertag']))[1:-1]\n",
    "    \n",
    "    # Format string for API\n",
    "    player_list = player_list.replace(', ',',')\n",
    "    player_list = player_list.replace(\"'\",'')\n",
    "    player_list = player_list.replace(' ','+')\n",
    "    \n",
    "    # Return in one full string\n",
    "    return player_list\n",
    "\n",
    "get_player_list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Player History\n",
    "\n",
    "With the gamertags prepared in one string, we'll call the Player Service Records - Arena API, which will return a single JSON file for each player detailing their aggregate stats for every variety of Arena game type (Slayer, Capture the Flag, Oddball, Strongholds, etc).  This will have informatino like total wins, total losses, total kills / assists / deaths all specific to each game type.\n",
    "\n",
    "While we'll never know for certain, the theory behind compiling the model dataframe by game type is that the features would be more representative of skill and experience in that game type.\n",
    "\n",
    "To further elaborate, we'll be using games exclusively played in Super Fiesta Party playlist, which respawns players with randomized weapons after every death.  Even if a player is very skilled at Halo, if they have never played this variety of gametype before, they will not likely fare as well as they do in more traditional Halo game types (at least for their first few games).  Thus, total stats for a player's extended Halo history would not be representative of their skill in the Super Fiesta Party playlist.\n",
    "\n",
    "We'll compile each player's service info in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:48.132210Z",
     "start_time": "2021-07-26T16:12:47.429708Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to pull more informative information about each player in the match\n",
    "# This information is not available in the two previous API calls\n",
    "def get_player_history(df, readable=False):\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Ocp-Apim-Subscription-Key': str(api_key),\n",
    "    }\n",
    "    params = urllib.parse.urlencode({\n",
    "    })\n",
    "    # Use our function in the block above the prepare the gamertags for the API\n",
    "    player_list_api = get_player_list(df)\n",
    "    \n",
    "    # Try calling service records API using our player list\n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('www.haloapi.com')\n",
    "        conn.request(\"GET\", f\"/stats/h5/servicerecords/arena?players={player_list_api}&%s\" % params, \"{body}\", headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "        player_history = json.loads(data)\n",
    "        conn.close()\n",
    "    \n",
    "    # Return error if issue with API\n",
    "    except Exception as e:\n",
    "        print(f\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    # Option to view in byte string readable format\n",
    "    if readable == False:\n",
    "        return player_history\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Show result\n",
    "player_history = get_player_history(df)\n",
    "# player_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build History Dataframe\n",
    "\n",
    "Now that we have our player service records for the single match, we'll extend our base dataframe by building out a new, more detailed 'variant' dataframe and append it to the base dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:48.259239Z",
     "start_time": "2021-07-26T16:12:48.134212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to build secondary dataframe with more informative player stats\n",
    "def build_history_dataframe(player_history, variant_id, streamlit=False):\n",
    "    \n",
    "    # Option to view 'streamlit' dataframe, which includes pertinent\n",
    "    # information but excludes all stats for modeling\n",
    "    if streamlit == True:\n",
    "        vdf_columns = ['Gamertag','TotalTimePlayed','K/D','Accuracy','WinRate']\n",
    "        vdf = pd.DataFrame(columns = vdf_columns)\n",
    "    else:\n",
    "        stat_list = ['Gamertag', 'TotalKills', 'TotalHeadshots', 'TotalWeaponDamage', 'TotalShotsFired',\n",
    "                    'TotalShotsLanded', 'TotalMeleeKills', 'TotalMeleeDamage', 'TotalAssassinations',\n",
    "                    'TotalGroundPoundKills', 'TotalGroundPoundDamage', 'TotalShoulderBashKills',\n",
    "                    'TotalShoulderBashDamage', 'TotalGrenadeDamage', 'TotalPowerWeaponKills',\n",
    "                    'TotalPowerWeaponDamage', 'TotalPowerWeaponGrabs', 'TotalPowerWeaponPossessionTime',\n",
    "                    'TotalDeaths', 'TotalAssists', 'TotalGamesCompleted', 'TotalGamesWon',\n",
    "                    'TotalGamesLost', 'TotalGamesTied', 'TotalTimePlayed','TotalGrenadeKills']\n",
    "        vdf = pd.DataFrame(columns = stat_list)\n",
    "    \n",
    "    # Set coutner variable\n",
    "    i = 0\n",
    "    # Loop the goes through each player in the player history JSON\n",
    "    for player in player_history['Results']:\n",
    "        \n",
    "        # Loop that goes through each Arena Game Base Variant and locates\n",
    "        # the details specific to the game vase variant of the match\n",
    "        for variant in player['Result']['ArenaStats']['ArenaGameBaseVariantStats']:\n",
    "            if variant['GameBaseVariantId'] == variant_id:\n",
    "                variant_stats = variant\n",
    "        \n",
    "        # Create empty dictionary where stats will be added\n",
    "        variant_dic = {}\n",
    "        \n",
    "        # Streamlit option - calculates specifc features\n",
    "        if streamlit == True:\n",
    "            variant_dic['Gamertag'] = player_history['Results'][i]['Id']\n",
    "            variant_dic['TotalTimePlayed']= isodate.parse_duration(variant_stats['TotalTimePlayed']).total_seconds() / 3600\n",
    "            variant_dic['K/D'] = variant_stats['TotalKills'] / variant_stats['TotalDeaths']\n",
    "            variant_dic['Accuracy'] = variant_stats['TotalShotsLanded'] / variant_stats['TotalShotsFired']\n",
    "            variant_dic['WinRate'] = variant_stats['TotalGamesWon'] / variant_stats['TotalGamesLost']\n",
    "            vdf = vdf.append(variant_dic, True)\n",
    "            i += 1\n",
    "        \n",
    "        # Modeling option - includes all features but does not yet calculate\n",
    "        else:\n",
    "            variant_dic['Gamertag'] = player_history['Results'][i]['Id']\n",
    "            \n",
    "            # Loop that appends all stats to variant dic\n",
    "            for stat in stat_list[1:]:    \n",
    "                variant_dic[stat] = variant_stats[stat]\n",
    "            \n",
    "            # Parsing ISO duration times\n",
    "            variant_dic['TotalTimePlayed']= isodate.parse_duration(variant_stats['TotalTimePlayed']).total_seconds() / 3600\n",
    "            vdf = vdf.append(variant_dic, True)\n",
    "            i += 1\n",
    "    \n",
    "    # Return the streamlit or modeling dataframe\n",
    "    return vdf\n",
    "    \n",
    "build_history_dataframe(player_history, '1571fdac-e0b4-4ebc-a73a-6e13001b71d3', streamlit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent Match Stats\n",
    "\n",
    "This function chains together all of the previous functions up to this point.  It returns a full dataframe for a single match, which could then be converted into a single row for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:51.501762Z",
     "start_time": "2021-07-26T16:12:48.261241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that combines all functions above to go through each step to\n",
    "# Get the match dataframe\n",
    "def recent_match_stats(gamertag, back_count=0):\n",
    "    \n",
    "    # Pull the match result as JSON from API\n",
    "    match_results = pull_recent_match(back_count, explore=False, gamertag=gamertag)\n",
    "    \n",
    "    # Build the base dataframe\n",
    "    base_df = build_base_dataframe(match_results, gamertag=gamertag)\n",
    "    \n",
    "    # Sleep for 1.01 seconds to avoid issues with API\n",
    "    time.sleep(1.01)\n",
    "    \n",
    "    # Create playerlist for player history API call\n",
    "    player_list = get_player_list(base_df)\n",
    "    \n",
    "    # Call API to get player history JSON\n",
    "    player_history = get_player_history(base_df)\n",
    "    \n",
    "    # Build base player stats dataframe based on player history API call\n",
    "    history_df = build_history_dataframe(player_history, match_results['GameBaseVariantId'])\n",
    "    \n",
    "    # Merge the base dataframe and stats dataframe\n",
    "    full_stats_df = pd.merge(base_df, history_df, how='inner', on = 'Gamertag')\n",
    "    \n",
    "    return full_stats_df\n",
    "\n",
    "# Show full dataframe for match\n",
    "df = recent_match_stats('Drymander', back_count=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Match Dataframe into Single Row\n",
    "\n",
    "Now we'll write a function to flatten the dataframe into a single row, which will be required for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:51.661799Z",
     "start_time": "2021-07-26T16:12:51.502762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert the full match dataframe into a single Pandas row for modeling\n",
    "def one_row(df, for_model=False):\n",
    "    \n",
    "    # If statement that rules out matches that will present issues for the model\n",
    "    # We want to make sure that exactly 8 players finished teh match and that\n",
    "    # No player exited the game before it was over\n",
    "    if ((for_model==True) and ((len(df.index) != 8) or (1 in df['DNF'].values))):\n",
    "        # Returns an empty dataframe that will be appended to the modeling dataset,\n",
    "        # effectively denoting that the match will not be usable for the model\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    # If the match meets the modeling criteria:\n",
    "    else:\n",
    "        # Sort by PlayerTeam (captures player team stats first)\n",
    "        # Sort by TotalTimePlayed\n",
    "        df = df.sort_values(by=['PlayerTeam', 'TotalTimePlayed'], ascending=(False, False))\n",
    "        \n",
    "        # Isolate portion of the dataframe for creating information we need\n",
    "        df = df.reset_index()\n",
    "        df_row = df.iloc[0:1,1:6]\n",
    "        \n",
    "        # Determine whether player won, lost, or tied the match\n",
    "        df_player = df.loc[df['PlayerTeam'] == 'Player']\n",
    "        if df_player['Winner'].str.contains('Victory').any():\n",
    "            df_row['WinLoseTie'] = 'Victory'\n",
    "        elif df_player['Winner'].str.contains('Defeat').any():\n",
    "            df_row['WinLoseTie'] = 'Defeat'\n",
    "        elif df_player['Winner'].str.contains('Tie').any():\n",
    "            df_row['WinLoseTie'] = 'Tie'\n",
    "        else: \n",
    "            df_row['WinLoseTie'] = 'Error Determining Victor'\n",
    "        \n",
    "        # 'Flatten' the match dataframe so that each player stat can\n",
    "        # be represented in one line of data\n",
    "        column_list = df.columns.to_list()\n",
    "        columns_converted = []\n",
    "        df = df.drop(df.iloc[:, 0:11], axis = 1)\n",
    "        df = df.stack().to_frame().T\n",
    "        df.columns = ['{}_{}'.format(*c) for c in df.columns]\n",
    "        \n",
    "        # Dictionary to convert strings denoting P1-4 (Player 1-4), E1 (Enemey 1-4)\n",
    "        column_convert_dic = {\"0_\":\"P1-\", \"1_\":\"P2-\",\"2_\":\"P3-\",\"3_\":\"P4-\",\n",
    "                              \"4_\":\"E1-\",\"5_\":\"E2-\",\"6_\":\"E3-\",\"7_\":\"E4-\",}\n",
    "\n",
    "        # Use dictionary to set column names\n",
    "        for k, v in column_convert_dic.items():\n",
    "            df.columns = df.columns.str.replace(k, v)\n",
    "        df.columns = df.columns.str.replace('-', '_')\n",
    "        df = df_row.join(df, how='outer')\n",
    "    \n",
    "    # Return match dataframe as one row\n",
    "    return df\n",
    "\n",
    "# Test function\n",
    "one_row(df, for_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Unique Gamertag List\n",
    "\n",
    "We're almost ready to start pulling rows into the dataframe.  We'll use this list exported from our EDA, which includes 24,248 unique players that I have personally played with over the past year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:51.669801Z",
     "start_time": "2021-07-26T16:12:51.662799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load unique gamertags pickle file from EDA notebook\n",
    "\n",
    "with open('data/unique_gamertags.pkl', 'rb') as unique_gamertags_pickle:\n",
    "    unique_gamertags = pickle.load(unique_gamertags_pickle)\n",
    "\n",
    "# See how many unique gamertags that the player has played with\n",
    "len(unique_gamertags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Pull Matches Function\n",
    "\n",
    "We're going to modify our Pull Recent Match function to pull 25 matches per player from our set of unique gamertags.  We're pulling 25 for a couple of reasons:\n",
    "\n",
    "- The API allows up to 25 Match ID's to be pulled at once\n",
    "- Many of these matches will not qualify for our model due to players leaving in the middle of the match\n",
    "- Many games will be of different game types that might not be relevant to the game type that we'll be modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:58.766883Z",
     "start_time": "2021-07-26T16:12:51.670801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that slightly models the pull_recent_match function\n",
    "# Designed to pull 25 matches from each gamertag for modeling\n",
    "def model_pull_matches(how_recent, api_key=api_key, \n",
    "                       gamertag='Drymander', count=25):\n",
    "    \n",
    "    # Use gamertag_for_api function to remove any spaces\n",
    "    gamertag = gamertag_for_api(gamertag)\n",
    "    # Set API key\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Ocp-Apim-Subscription-Key': api_key,\n",
    "    }\n",
    "    \n",
    "    # Pulls from arena mode, how_recent is how far to go back in the match history\n",
    "    # 'count' refers to the number of matches to pull\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters\n",
    "        'modes': 'arena',\n",
    "        'start': 0,\n",
    "        'count': count,\n",
    "        'include-times': True,\n",
    "    })\n",
    "    \n",
    "    # Try / except for error handling\n",
    "    try:\n",
    "        \n",
    "        # Connect to API and pull most recent 25 matches for specified gamer\n",
    "        # and format into JSON\n",
    "        conn = http.client.HTTPSConnection('www.haloapi.com')\n",
    "        conn.request(\"GET\", f\"/stats/h5/players/{gamertag}/matches?%s\" % params, \"{body}\", headers)\n",
    "        time.sleep(1.01)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "        history_pull = json.loads(data)\n",
    "        print(history_pull['ResultCount'])\n",
    "        \n",
    "        # Counter variable for printing status\n",
    "        i = 0\n",
    "        \n",
    "        # Empty list to append 25 matches\n",
    "        latest_count_matches = []\n",
    "        \n",
    "        # Loop to go through each of the 25 matches to pull match details for each\n",
    "        for match in history_pull['Results']:\n",
    "            \n",
    "            # Identify match ID and match date\n",
    "            match_id = match['Id']['MatchId']\n",
    "            match_date = match['MatchCompletedDate']['ISO8601Date']\n",
    "            \n",
    "            # API call for each match ID in teh 25 matches\n",
    "            conn.request(\"GET\", f\"/stats/h5/arena/matches/{match_id}?%s\" % params, \"{body}\", headers)\n",
    "            time.sleep(1.1)\n",
    "            \n",
    "            # Format into JSON and append match ID and date\n",
    "            response = conn.getresponse()\n",
    "            data = response.read()\n",
    "            match_results = json.loads(data)\n",
    "            match_results['MatchId'] = match_id\n",
    "            match_results['Date'] = match_date\n",
    "            \n",
    "            # Append each match JSON to full list\n",
    "            latest_count_matches.append(match_results)\n",
    "            conn.close()\n",
    "            i += 1\n",
    "            \n",
    "            # Print total number of matches appended to list\n",
    "            print(f'{i} matches appended')\n",
    "    \n",
    "    # Error handling\n",
    "    except Exception as e:\n",
    "        print(f\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    # Return full set of 25 matches\n",
    "    return latest_count_matches\n",
    "\n",
    "# Testing the function\n",
    "latest_count_matches = model_pull_matches(0, gamertag='Drymander', count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Rows to Dataframe\n",
    "\n",
    "Finally, this function will chain everything together to add rows to the modeling dataframe.  \n",
    "\n",
    "You might notice that the gamertag list is chunked at [651:850].  This is where I left off with pulling rows, but I can continue adding more rows should it prove more beneficial to the model.  651 is the start number in the unique gamer tags list, and 850 is the end number.  This means I have pulled 25 matches from 850 gamertags, but it should be noted that some or all of those matches might not have been added to the final modeling dataframe.  The reasons for this incldue:\n",
    "- Less than 8 players (4 on each team) finished the match.\n",
    "- 8 players finished the match, but one or more players other than the 8 finishers disconnected from the match and was replaced mid-game.\n",
    "- During the Player Service Record API pull, one or more of the 8 players changed their gamertag **after** the date of the match, meaning the gamertag returned null values for their service record.\n",
    "\n",
    "This function is designed to append rows to a .csv file as it runs.  Since the function requires multiple API calls per row, this allows the ability to stop / start the function as needed.  It also allows you to pull additional rows in manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:12:58.776886Z",
     "start_time": "2021-07-26T16:12:58.767884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting run to false\n",
    "# If set to true, model will start adding new rows to model datafarme stored in .csv\n",
    "run = False\n",
    "\n",
    "# Isolating portion of unique_gamertags to build model dataframe in manageable chunks\n",
    "gamertag_list = unique_gamertags[651:850]\n",
    "\n",
    "# Function add rows to the modeling dataframe\n",
    "def model_recent_match_stats(gamertag_list, back_count=0, count=25):\n",
    "    \n",
    "    # Create new dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Set gamertag_count to zero, will be used in updating status via print\n",
    "    gamertag_count = 0\n",
    "    \n",
    "    # Loop through gamertags in unique_gamertags list\n",
    "    for gamertag in tqdm(gamertag_list):\n",
    "        \n",
    "        # Try / except to deal with API error handling\n",
    "        try:\n",
    "            # Use latest_count_match function to pull 25 matches from player \n",
    "            latest_count_matches = model_pull_matches(0, gamertag=gamertag, count=count)\n",
    "            time.sleep(1.1)\n",
    "            \n",
    "            # Setting error counter and additional counter variable\n",
    "            error_count = 0\n",
    "            i = 0\n",
    "            \n",
    "            # Loop through each of the players 25 matches\n",
    "            for match in latest_count_matches:\n",
    "                \n",
    "                # Error handling\n",
    "                try:\n",
    "                    \n",
    "                    # Build the base dataframe\n",
    "                    base_df = build_base_dataframe(match, gamertag=gamertag)\n",
    "                    \n",
    "                    # Create playerlist for player history API call\n",
    "                    player_list = get_player_list(base_df)\n",
    "                    \n",
    "                    # Call API to get player history JSON\n",
    "                    player_history = get_player_history(base_df)\n",
    "                    \n",
    "                    # Build base player stats dataframe based on player history API call\n",
    "                    history_df = build_history_dataframe(player_history, match_results['GameBaseVariantId'])\n",
    "                    \n",
    "                    # Merge the base dataframe and stats dataframe\n",
    "                    full_stats_df = pd.merge(base_df, history_df, how='inner', on = 'Gamertag')\n",
    "                    \n",
    "                    # Flatten full match dataframe into one row, \n",
    "                    row = one_row(full_stats_df, for_model=True)\n",
    "                    \n",
    "                    # Append row to model dataframe .csv with specific date format  \n",
    "                    row.to_csv('data/MODEL_PULL.csv', mode ='a', date_format='%Y-%m-%d %H:%M:%S', header=False)\n",
    "                    \n",
    "                    # Append to model dataframe if working outside of .csv\n",
    "                    df = df.append(row)\n",
    "                    \n",
    "                    # Print how many rows have been added to the dataframe\n",
    "                    i += 1\n",
    "                    print(f'{i} rows added to model dataframe')\n",
    "\n",
    "                    time.sleep(1.1)\n",
    "                except:\n",
    "                    \n",
    "                    # Print error count if row cannot be added because it doesn't meet criteria\n",
    "                    # Typically this occurs when a player has changed their gamertag\n",
    "                    error_count += 1\n",
    "                    print(f'{error_count} rows returned error when getting player history')\n",
    "                    time.sleep(1.1)\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "            \n",
    "            # Print number of gamertags that the function has gone through\n",
    "            gamertag_count += 1\n",
    "            print(f'{gamertag_count} completed')\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            # Show error message if gamer skipped due to name change or other issue with API\n",
    "            print('gamertag skipped due to error')\n",
    "    \n",
    "    # Return modeling dataframe\n",
    "    return df\n",
    "\n",
    "if run == True:\n",
    "    model_df = model_recent_match_stats(gamertag_list, back_count=0, count=25)\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Dataframe from CSV\n",
    "\n",
    "Let's take a look at our modeling dataframe by loading the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:01.222439Z",
     "start_time": "2021-07-26T16:12:58.778887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load csv created by model_recent_match_stats\n",
    "df = pd.read_csv('data/MODEL_PULL.csv')\n",
    "\n",
    "# Convert the dates to datetime objects\n",
    "df['Date'] = df['Date'].apply(pd.to_datetime)\n",
    "\n",
    "# Drop 'Unnamed: 0' from dataframe\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:02.208662Z",
     "start_time": "2021-07-26T16:13:01.223440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove null, infinity, and negative infinity\n",
    "# This can cause errors when creating features\n",
    "len_df = len(df)\n",
    "print(f'There are {len_df} total rows in the model dataframe.')\n",
    "print(f'There are {len(df[df.isin([np.nan, np.inf, -np.inf]).any(1)])} null or infinity values that should be removed from the model.')\n",
    "df = df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "len_df = len(df)\n",
    "print(f'There are {len_df} rows after removing null and infinity values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power weapon possession time needs to be converted from ISO duration times to floats representing hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:04.430738Z",
     "start_time": "2021-07-26T16:13:02.209662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define roster to interact with dataframe\n",
    "roster = ['P1', 'P2', 'P3', 'P4', 'E1', 'E2', 'E3', 'E4']\n",
    "\n",
    "# Loop through players in roster\n",
    "for player in roster:\n",
    "    \n",
    "    # Empty list of parsed times\n",
    "    parsed_times = []\n",
    "    \n",
    "    # Parse times for each player \n",
    "    for row in df[f'{player}_TotalPowerWeaponPossessionTime']:\n",
    "        row = isodate.parse_duration(row).total_seconds() / 3600\n",
    "        parsed_times.append(row)\n",
    "    \n",
    "    # Set column to parsed times list\n",
    "    df[f'{player}_TotalPowerWeaponPossessionTime'] = parsed_times\n",
    "\n",
    "df['P1_TotalPowerWeaponPossessionTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our EDA, we'll need to decode a few columns for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:04.435742Z",
     "start_time": "2021-07-26T16:13:04.431738Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function will convert codes provided by the API into a readable format\n",
    "def decode_column(df, column, api_dict):\n",
    "    \n",
    "    # Empty list of decoded values\n",
    "    decoded_list = []\n",
    "    \n",
    "    # Loop through each row\n",
    "    for row in df[column]:\n",
    "        i = 0\n",
    "        \n",
    "        # Loop through API dictionary\n",
    "        for item in api_dict:\n",
    "            \n",
    "            # If code found, append it to list\n",
    "            if item['id'] == row:\n",
    "                name = item['name']\n",
    "                decoded_list.append(name)\n",
    "            \n",
    "            # Otherwise keep searching until found\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    # Return decoded list\n",
    "    return decoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:04.448743Z",
     "start_time": "2021-07-26T16:13:04.436740Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function will convert maps to readable format\n",
    "def decode_maps(df, column, api_dict):\n",
    "    decoded_list = []\n",
    "    \n",
    "    # Loop through each row\n",
    "    for row in df[column]:\n",
    "        i = 0\n",
    "        \n",
    "        # Creating map_count variable\n",
    "        map_count = len(api_dict)\n",
    "        \n",
    "        # For each item in API dictionary\n",
    "        for item in api_dict:\n",
    "            \n",
    "            # If map cannot be found, name 'Custom Map'\n",
    "            if (i+1) == map_count:\n",
    "                name = 'Custom Map'\n",
    "                decoded_list.append(name)\n",
    "            \n",
    "            # If found, assign value to code\n",
    "            elif item['id'] == row:\n",
    "                name = item['name']\n",
    "                decoded_list.append(name)\n",
    "            \n",
    "            # Otherwise keep looping\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    # Return decoded list\n",
    "    return decoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:05.834058Z",
     "start_time": "2021-07-26T16:13:04.449743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading GameBaseVariantId metadata dictionary pulled from API\n",
    "with open('data/GameBaseVariantId.pkl', 'rb') as GameBaseVariantId_pickle:\n",
    "    GameBaseVariantId_dic = pickle.load(GameBaseVariantId_pickle)\n",
    "\n",
    "# Loading PlaylistId metadata dictionary pulled from API\n",
    "with open('data/PlaylistId_dic.pkl', 'rb') as PlaylistId_dic_pickle:\n",
    "    PlaylistId_dic = pickle.load(PlaylistId_dic_pickle)\n",
    "\n",
    "# Loading map_list metadata dictionary pulled from API\n",
    "with open('data/map_list.pkl', 'rb') as map_list_pickle:\n",
    "    map_list = pickle.load(map_list_pickle)\n",
    "\n",
    "# Decode columsn with using our decode functions\n",
    "df['GameBaseVariantId'] = decode_column(df, 'GameBaseVariantId', GameBaseVariantId_dic)    \n",
    "df['PlaylistId'] = decode_column(df, 'PlaylistId', PlaylistId_dic)\n",
    "df['MapVariantId'] = decode_maps(df, 'MapVariantId', map_list)\n",
    "\n",
    "df[['GameBaseVariantId', 'PlaylistId', 'MapVariantId']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will:\n",
    "- Remove tie games\n",
    "- Ensure victory and defeat are represented as 1 and 0 integers\n",
    "- Rename WinLoseTie to PlayerWin, which better describes our model target\n",
    "- Filter our match date to ensure our match information and our player service record information are chronologically synced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:05.991093Z",
     "start_time": "2021-07-26T16:13:05.835059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove ties from df\n",
    "df = df[df['WinLoseTie'] != 'Tie']\n",
    "\n",
    "# Set victories to 1 and defeats to 0\n",
    "df.loc[(df['WinLoseTie'] == 'Victory'),'WinLoseTie'] = 1\n",
    "df.loc[(df['WinLoseTie'] == 'Defeat'),'WinLoseTie'] = 0\n",
    "\n",
    "# Convert to integers to be safe\n",
    "df['WinLoseTie'] = df['WinLoseTie'].astype('int')\n",
    "\n",
    "# Rename WinLoseTie to PlayerWin for clarity\n",
    "df.rename(columns={'WinLoseTie':'PlayerWin'}, inplace=True)\n",
    "\n",
    "# Set date range, only want matches later than 7/1/21\n",
    "print(len(df))\n",
    "df = df[(df['Date'] > '2021-07-01')]\n",
    "print(len(df))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose GameBaseVariantId to model\n",
    "\n",
    "This function was used a bit more during experimentation, but it is still useful for filtering our model data by game base variant (Slayer or Capture the Flag) and playlist (Super Fiesta Party).  \n",
    "\n",
    "Since the majority of games I have played were Super Fiesta Party, we'll focus on those games for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:06.008097Z",
     "start_time": "2021-07-26T16:13:05.992094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to choose gametype\n",
    "def choose_gametype(df, GameBaseVariantId, PlaylistId):\n",
    "    \n",
    "    # If none selected, return df\n",
    "    # This will be useful for the next function\n",
    "    if GameBaseVariantId == None and PlaylistId == None:\n",
    "        gametype_df = df\n",
    "    \n",
    "    # Option to set GameBaseVariantId to None\n",
    "    elif GameBaseVariantId == None:\n",
    "        gametype_df = df[df['PlaylistId'] == PlaylistId]\n",
    "    \n",
    "    # Option to set PlaylistId to None\n",
    "    elif PlaylistId == None:\n",
    "        gametype_df = df[df['GameBaseVariantId'] == GameBaseVariantId]\n",
    "    \n",
    "    # Set dataframe to specified GameBaseVariantId and PlaylistId\n",
    "    else:\n",
    "        gametype_df = df[(df['GameBaseVariantId'] == GameBaseVariantId) & (df['PlaylistId'] == PlaylistId)]\n",
    "    \n",
    "    # Return dataframe\n",
    "    return gametype_df\n",
    "\n",
    "# Set to Super Fiesta Party\n",
    "df = choose_gametype(df, 'Capture the Flag', 'Super Fiesta Party')\n",
    "\n",
    "# Check function with value counts\n",
    "df['PlaylistId'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns\n",
    "\n",
    "We'll drop columns that will not be helpful for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:06.130125Z",
     "start_time": "2021-07-26T16:13:06.009097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop(['Date',\n",
    "        'MatchId',\n",
    "        'GameBaseVariantId',\n",
    "        'PlaylistId',\n",
    "        'MapVariantId',\n",
    "        'P1_Gamertag',\n",
    "        'P2_Gamertag',\n",
    "        'P3_Gamertag',\n",
    "        'P4_Gamertag',\n",
    "        'E1_Gamertag',\n",
    "        'E2_Gamertag',\n",
    "        'E3_Gamertag',\n",
    "        'E4_Gamertag',\n",
    "        ]\n",
    "        ,axis=1)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "\n",
    "Done in a slighty different order than the EDA, we will still want to create features that might be helpful for our model.\n",
    "\n",
    "We'll be creating:\n",
    "- Win rate\n",
    "- K/D\n",
    "- Accuracy\n",
    "\n",
    "And we'll also convert all total lifetime game base variant stats into 'per game' stats.  These per game stats might be more indicative of skill, whereas total lifetime stats are more indicative of experience.  Both are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:13:06.678128Z",
     "start_time": "2021-07-26T16:13:06.134125Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set roster for sifting through players of dataframe\n",
    "roster = ['P1', 'P2', 'P3', 'P4', 'E1', 'E2', 'E3', 'E4']\n",
    "\n",
    "# Loop through players in roster\n",
    "for player in roster:\n",
    "    \n",
    "    # Set win rate\n",
    "    df[f'{player}_WinRate'] = df[f'{player}_TotalGamesWon'] / df[f'{player}_TotalGamesLost']\n",
    "    \n",
    "    # Set K/D (or Kill / Death ratio)\n",
    "    df[f'{player}_K/D'] = df[f'{player}_TotalKills'] / df[f'{player}_TotalDeaths']\n",
    "    \n",
    "    # Set accuracy\n",
    "    df[f'{player}_Accuracy'] = df[f'{player}_TotalShotsLanded'] / df[f'{player}_TotalShotsFired']\n",
    "    \n",
    "    per_game_stat_list = ['TotalKills', 'TotalHeadshots', 'TotalWeaponDamage', \n",
    "                      'TotalShotsFired', 'TotalShotsLanded', 'TotalMeleeKills', \n",
    "                      'TotalMeleeDamage', 'TotalAssassinations', 'TotalGroundPoundKills', \n",
    "                      'TotalGroundPoundDamage', 'TotalShoulderBashKills', \n",
    "                      'TotalShoulderBashDamage', 'TotalGrenadeDamage', 'TotalPowerWeaponKills', \n",
    "                      'TotalPowerWeaponDamage', 'TotalPowerWeaponGrabs', \n",
    "                      'TotalPowerWeaponPossessionTime', 'TotalDeaths', 'TotalAssists', \n",
    "                      'TotalGrenadeKills']\n",
    "            \n",
    "    for stat in per_game_stat_list:\n",
    "        per_game_stat_string = stat.replace('Total', '')\n",
    "        per_game_stat_string = f'{per_game_stat_string}PerGame'\n",
    "        df[f'{player}_{per_game_stat_string}'] = df[f'{player}_{stat}'] / df[f'{player}_TotalGamesCompleted']\n",
    "#         variant_dic[per_game_stat_string] = variant_dic[stat] / variant_dic['TotalGamesCompleted']\n",
    "\n",
    "\n",
    "# Drop infinity values, which can arise if it is the first time a player\n",
    "# is playing specified playlist\n",
    "df = df.dropna()\n",
    "df = df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Model Dataframe Function\n",
    "\n",
    "Something that will be helpful for our model will be sorting each feature from highest to lowest for each team of players.  For each column in feature_list, this function will take each stat for P1, P2, P3, and P4 and reorder the stats.  Hopefully this will help the model understand that P1 represents the 'best' or 'highest' stat on that team, and P4 represents the 'worst' or 'lowest.'  The function will do the same for the enemy team (E1, E2, E3, and E4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:45.981035Z",
     "start_time": "2021-07-26T16:13:06.680127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set columns for full model dataframe\n",
    "feature_list = [\n",
    "    # Main stats\n",
    "    'WinRate', 'K/D', 'Accuracy', 'TotalGamesCompleted', 'TotalGamesWon',\n",
    "    'TotalGamesLost', 'PrevTotalXP', 'SpartanRank', 'TotalTimePlayed',\n",
    "    # Total life time stats\n",
    "#     'TotalKills', 'TotalHeadshots', 'TotalWeaponDamage', \n",
    "#     'TotalShotsFired', 'TotalPowerWeaponPossessionTime',\n",
    "#     'TotalShotsLanded', 'TotalMeleeKills', 'TotalMeleeDamage', 'TotalAssassinations',\n",
    "#     'TotalGroundPoundKills', 'TotalGroundPoundDamage', 'TotalShoulderBashKills',\n",
    "#     'TotalShoulderBashDamage', 'TotalGrenadeDamage', 'TotalPowerWeaponKills',\n",
    "#     'TotalPowerWeaponDamage', 'TotalPowerWeaponGrabs',\n",
    "#     'TotalDeaths', 'TotalAssists', 'TotalGamesTied', 'TotalGrenadeKills',\n",
    "    # Per game stats   \n",
    "    'KillsPerGame', 'HeadshotsPerGame', 'WeaponDamagePerGame', \n",
    "    'ShotsFiredPerGame', 'ShotsLandedPerGame', 'MeleeKillsPerGame', \n",
    "    'MeleeDamagePerGame', 'AssassinationsPerGame', 'GroundPoundKillsPerGame', \n",
    "    'GroundPoundDamagePerGame', 'ShoulderBashKillsPerGame', \n",
    "    'ShoulderBashDamagePerGame', 'GrenadeDamagePerGame', 'PowerWeaponKillsPerGame', \n",
    "    'PowerWeaponDamagePerGame', 'PowerWeaponGrabsPerGame', \n",
    "    'PowerWeaponPossessionTimePerGame', 'DeathsPerGame', 'AssistsPerGame', \n",
    "    'GrenadeKillsPerGame',\n",
    "]\n",
    "\n",
    "\n",
    "# Function that sorts player stats\n",
    "def sort_players(df, feature_list, GameBaseVariantId, PlaylistId):\n",
    "    \n",
    "    # Choose gametype function\n",
    "    df = choose_gametype(df, GameBaseVariantId, PlaylistId) \n",
    "    \n",
    "    # Empty dataframe with PlayerWin as first column\n",
    "    model_df = pd.DataFrame()\n",
    "    model_df['PlayerWin'] = df['PlayerWin']\n",
    "    \n",
    "    # Loop that sorts player stats per team\n",
    "    for feature in feature_list:\n",
    "        feature_columns = [\n",
    "            f'P1_{feature}', f'P2_{feature}',\n",
    "            f'P3_{feature}', f'P4_{feature}', f'E1_{feature}',\n",
    "            f'E2_{feature}', f'E3_{feature}', f'E4_{feature}',\n",
    "            ]\n",
    "        \n",
    "        # Copy input dataframe columns\n",
    "        feature_df = df[feature_columns].copy()\n",
    "\n",
    "        # Sort Players in dataframe by highest value\n",
    "        i = 0\n",
    "        for row in tqdm(feature_df.iterrows()):\n",
    "            # Sort player / enemy from highest to lowest in row\n",
    "            feature_df.iloc[i, 0:4] = feature_df.iloc[i, 0:4].sort_values(ascending=False).values\n",
    "            feature_df.iloc[i, 4:8] = feature_df.iloc[i, 4:8].sort_values(ascending=False).values\n",
    "            i += 1\n",
    "        \n",
    "        # Join sorted features with PlayerWin column\n",
    "        model_df = model_df.join(feature_df, on=model_df.index)\n",
    "    \n",
    "    # Drop null values\n",
    "    model_df = model_df.dropna()\n",
    "    \n",
    "    # Return sorted dataframe\n",
    "    return model_df\n",
    "            \n",
    "df = sort_players(df, feature_list, None, None)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:45.984036Z",
     "start_time": "2021-07-26T16:15:45.982035Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_csv('data/Model_DF_PerGameFeatures_Sorted_HuskyRaid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with All Features\n",
    "\n",
    "We'll start by modeling all features broken down and sorted by individual player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:46.192082Z",
     "start_time": "2021-07-26T16:15:45.985036Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Model_DF_PerGameFeatures_Sorted_HuskyRaid.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:46.214087Z",
     "start_time": "2021-07-26T16:15:46.193082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make model_df from a copy of our dataframe up to this point\n",
    "model_df = df.copy()\n",
    "\n",
    "# Assign features and target\n",
    "features = model_df.drop(['PlayerWin'], axis=1)\n",
    "target = model_df['PlayerWin']\n",
    "\n",
    "# Assigning X and y for train test split\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# Ensure target is integer format\n",
    "y = y.astype('int')\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=8)\n",
    "\n",
    "# Print shape\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a basic linear regression for the purposes of creating and testing our 'evaluate model' and 'make model' functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:46.326112Z",
     "start_time": "2021-07-26T16:15:46.215087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create logistic regression model\n",
    "model_log = LogisticRegression(random_state=8)\n",
    "\n",
    "# Train on X_train and y_train\n",
    "model_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will return cross validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:46.671190Z",
     "start_time": "2021-07-26T16:15:46.327112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to return cross validation scores\n",
    "def cross_val_check(model_string_name, model, X_train, y_train, X_test, y_test):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5) # model, train, target, cross validation\n",
    "    print(f'{model_string_name} Cross Validation Scores:')\n",
    "    print(scores)\n",
    "    print(f'\\nCross validation mean: \\t{scores.mean():.2%}')\n",
    "    \n",
    "cross_val_check('Logistic Regression', model_log, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will return performance information and helpful visuals for interpreting the strengths and weaknesses of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:47.125293Z",
     "start_time": "2021-07-26T16:15:46.672190Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, \n",
    "                   y_test, cmap='Greens', normalize=None,\n",
    "                   classes=None,figsize=(10,4), graphs=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    model :: classifier user desires to evaluate\n",
    "    X_train :: X training data\n",
    "    X_test :: X test data\n",
    "    y_train :: y_train data\n",
    "    y_test :: y_train data\n",
    "    cmap :: color palette of confusion matrix\n",
    "    normalize :: set to True if normalized confusion matrix is desired\n",
    "    figsize :: desired plot size\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Print model accuracy\n",
    "    print(f'Training Accuracy: {model.score(X_train,y_train):.2%}')\n",
    "    print(f'Test Accuracy: {model.score(X_test,y_test):.2%}')\n",
    "    print('')\n",
    "    \n",
    "    # Option to show graphs\n",
    "    if graphs == True:\n",
    "        \n",
    "        # Print classification report\n",
    "        y_test_predict = model.predict(X_test)\n",
    "        print(metrics.classification_report(y_test, y_test_predict,\n",
    "                                            target_names=classes))\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        fig,ax = plt.subplots(ncols=2,figsize=figsize)\n",
    "        metrics.plot_confusion_matrix(model, X_test,y_test,cmap=cmap, \n",
    "                                      normalize=normalize,display_labels=classes,\n",
    "                                      ax=ax[0])\n",
    "\n",
    "        #Plot ROC curves\n",
    "        \n",
    "        with sns.axes_style(\"darkgrid\"):\n",
    "            curve = metrics.plot_roc_curve(model,X_train,y_train,ax=ax[1])\n",
    "            curve2 = metrics.plot_roc_curve(model,X_test,y_test,ax=ax[1])\n",
    "            curve.ax_.grid()\n",
    "            curve.ax_.plot([0,1],[0,1],ls=':')\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "evaluate_model(model_log, X_train, X_test, y_train, \n",
    "                   y_test, graphs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build a function that accomplishes a few things:\n",
    "- Conducts the full train / test split process\n",
    "- Models the data\n",
    "- Provides evaluation metrics\n",
    "- Has options for choosing model type (e.g. logistic regression, random forest, SVM, XGBoost, etc.)\n",
    "- Has an option for running a dummy model\n",
    "- Has option for choosing a scaler if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:48.034501Z",
     "start_time": "2021-07-26T16:15:47.126292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create models\n",
    "def make_model(df, regressor=LogisticRegression, scale=False, graphs=False, dummy=False, cmap='Greens',\n",
    "              slim=False, scaler=StandardScaler()):\n",
    "\n",
    "    # Assigning X and y for train test split\n",
    "    X = df.drop(['PlayerWin'], axis=1)\n",
    "    y = df['PlayerWin']\n",
    "    \n",
    "    # Ensure target is integer format\n",
    "    y=y.astype('int')\n",
    "\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                        random_state=8)\n",
    "    \n",
    "    # Option to scale data with scaler type as parameter\n",
    "    if scale==True:\n",
    "        scaler = scaler\n",
    "        X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "        X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "    \n",
    "    # Option to return dummy model\n",
    "    if dummy == True:\n",
    "        model_log = DummyClassifier(strategy='stratified')\n",
    "        print('Using Dummy Model')\n",
    "    else:\n",
    "        model_log = regressor\n",
    "    \n",
    "    # Fit to X_train and y_train\n",
    "    model_log.fit(X_train, y_train)\n",
    "   \n",
    "    # Print total number of samples\n",
    "    total_samples = X_train.shape[0] + X_test.shape[0]\n",
    "    print(f'Total number of samples: {total_samples}')\n",
    "    print('------------------------------------------')\n",
    "    \n",
    "    # Option to suppress cross validation scores\n",
    "    if slim == False:\n",
    "        cross_val_check(str(regressor), model_log, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    evaluate_model(model_log, X_train, X_test, y_train, y_test, graphs=graphs, cmap=cmap)\n",
    "\n",
    "make_model(model_df, scale=False, graphs=True, regressor=LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Now that we have a function to fully model our data, we can try our data on a few different types of models.  We'll start with basic logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:48.479600Z",
     "start_time": "2021-07-26T16:15:48.035501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic logistic regression\n",
    "make_model(df, regressor=LogisticRegression(random_state=8), scale=False, graphs=False, cmap='Greens',\n",
    "              slim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if using scalers makes a difference.  We'll try Power Transformer, Standard Scaler, and Robust Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:51.507287Z",
     "start_time": "2021-07-26T16:15:48.480601Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression with PowerTransformer')\n",
    "make_model(model_df, scale=True, graphs=False, regressor=LogisticRegression(), \n",
    "           scaler=PowerTransformer())\n",
    "print('Logistic Regression with StandardScaler')\n",
    "make_model(model_df, scale=True, graphs=False, regressor=LogisticRegression(), \n",
    "           scaler=StandardScaler())\n",
    "print('Logistic Regression with RobustScaler')\n",
    "make_model(model_df, scale=True, graphs=False, regressor=LogisticRegression(), \n",
    "           scaler=RobustScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the power transformer is best for logistic regression, let's take a look at the full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:53.722787Z",
     "start_time": "2021-07-26T16:15:51.508287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic regression with power transformer, full evalutation\n",
    "make_model(model_df, scale=True, graphs=True, regressor=LogisticRegression(), scaler=PowerTransformer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to reach 70.29% test accuracy.  Let's run a dummy model to see if this score is better than a dummy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:15:54.171888Z",
     "start_time": "2021-07-26T16:15:53.723787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dummy model for comparison\n",
    "make_model(model_df, scale=False, graphs=True, dummy=True, \n",
    "           cmap='Reds', regressor=LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a test accuracy of 51.72%, our logistic regression model with power transformer at 70.29% test accuracy is at least picking up on something.  Before diving into what it's using to make its predictions, let's try a few more models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Next we'll try random forest models to see how they perform, with and without scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:16:09.302324Z",
     "start_time": "2021-07-26T16:15:54.172889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random forest not scaled\n",
    "make_model(model_df, scale=False, graphs=False, regressor=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:16:55.881692Z",
     "start_time": "2021-07-26T16:16:09.303325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random forest with different scales\n",
    "print('Random Forest with PowerTransformer')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=RandomForestClassifier(random_state=8), scaler=PowerTransformer())\n",
    "print('Random Forest with StandardScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=RandomForestClassifier(random_state=8), scaler=StandardScaler())\n",
    "print('Random Forest with RobustScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=RandomForestClassifier(random_state=8), scaler=RobustScaler())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were almost able to crack 72% with the power tranformer.  Let's take a look at the full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:17:12.714127Z",
     "start_time": "2021-07-26T16:16:55.882692Z"
    }
   },
   "outputs": [],
   "source": [
    "make_model(model_df, scale=True, graphs=True, \n",
    "           regressor=RandomForestClassifier(random_state=8), scaler=PowerTransformer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest model with the power transformer is our best model so far at 71.61% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Next we'll take a look at support vector machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:17:27.474575Z",
     "start_time": "2021-07-26T16:17:12.715127Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVM without scaling\n",
    "make_model(model_df, scale=False, graphs=False, regressor=svm.SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:18:08.572475Z",
     "start_time": "2021-07-26T16:17:27.475576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Support Vector Machines with different scalers\n",
    "print('Support Vector Machine with PowerTransformer')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=svm.SVC(), scaler=PowerTransformer())\n",
    "print('Support Vector Machine with StandardScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=svm.SVC(), scaler=StandardScaler())\n",
    "print('Support Vector Machine with RobustScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=svm.SVC(), scaler=RobustScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We came pretty close to 72% with SVM using the power transformer, but random forest is still our best model.  Let's take a look at SVM using power transformer, which scored 71.48% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:18:27.762625Z",
     "start_time": "2021-07-26T16:18:08.573476Z"
    }
   },
   "outputs": [],
   "source": [
    "make_model(model_df, scale=True, graphs=True, \n",
    "           regressor=svm.SVC(), scaler=PowerTransformer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Finally, let's take a look at XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:18:33.524923Z",
     "start_time": "2021-07-26T16:18:27.763624Z"
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost with no scaler\n",
    "make_model(model_df, scale=False, graphs=False, regressor=XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:18:52.257146Z",
     "start_time": "2021-07-26T16:18:33.525923Z"
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost with different scalers\n",
    "print('XGBoost with PowerTransformer')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=XGBClassifier(random_state=8), scaler=PowerTransformer())\n",
    "print('XGBoost with StandardScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=XGBClassifier(random_state=8), scaler=StandardScaler())\n",
    "print('XGBoost with RobustScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=XGBClassifier(random_state=8), scaler=RobustScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While worth a shot, XGBoost did not outperform our SVM model.  While most models look nearly identical in terms of accuracy, let's take a closer look at XGBoost with robust scaler, since it has the highest cross validation mean at 71.54%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:18:58.307510Z",
     "start_time": "2021-07-26T16:18:52.258146Z"
    }
   },
   "outputs": [],
   "source": [
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=XGBClassifier(), scaler=RobustScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis\n",
    "\n",
    "Let's take a look at how logistic regression values different features in our full model dataset.  We'll create a model accuracy function and a plot coefficients function to explore the importances.\n",
    "\n",
    "Before that, we'll redo our train test split using the power tranformer, which was best for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:18:59.775848Z",
     "start_time": "2021-07-26T16:18:58.308511Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make model_df from a copy of our dataframe up to this point\n",
    "model_df = df.copy()\n",
    "\n",
    "# Assign features and target\n",
    "features = model_df.drop(['PlayerWin'], axis=1)\n",
    "target = model_df['PlayerWin']\n",
    "\n",
    "# Assigning X and y for train test split\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# Ensure target is integer format\n",
    "y = y.astype('int')\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=8)\n",
    "\n",
    "# Define and scale X train and test\n",
    "scaler = PowerTransformer()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "# Create and fit logistic regressoin\n",
    "model_log = LogisticRegression()\n",
    "model_log.fit(X_train, y_train)\n",
    "\n",
    "# Print shape\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our functions and take a look a the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:00.348974Z",
     "start_time": "2021-07-26T16:18:59.776848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to show model accuracy\n",
    "def model_accuracy(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    print(f'Training Accuracy: {model.score(X_train,y_train):.2%}')\n",
    "    print(f'Test Accuracy: {model.score(X_test,y_test):.2%}')\n",
    "\n",
    "# Uncomment line below to create images for presentation\n",
    "# sns.set_context('talk')\n",
    "\n",
    "# Function to plot logistic regression coefficients\n",
    "def plot_coefficients(model, features, X_train, X_test, y_train, y_test, count=20):    \n",
    "    \n",
    "    # Create a list of coefficients\n",
    "    coeffs = pd.Series(model.coef_.flatten(), index=features.columns).sort_values(ascending=False)\n",
    "#     coeffs = coeffs[:20]\n",
    "    top_coeffs = coeffs[:count]\n",
    "    bottom_coeffs = coeffs[-count:]\n",
    "    coeffs = top_coeffs.append(bottom_coeffs)\n",
    "    \n",
    "    # Display accuracy of newly trained model\n",
    "    model_accuracy(model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "\n",
    "    # Create coefficients plot\n",
    "    with sns.plotting_context(\"talk\", font_scale=1.4):\n",
    "        with sns.axes_style(\"darkgrid\"):\n",
    "            plt.figure(figsize=(16, 12))\n",
    "            ax = sns.barplot(x=coeffs, y=coeffs.index, palette='coolwarm')\n",
    "            ax.set(xlabel='Log Coefficients', ylabel='Features')\n",
    "            ax.set_title(\"Feature Importances\",fontsize=30)\n",
    "    \n",
    "    # Save image\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "plot_coefficients(model_log, features, X_train, X_test, y_train, y_test, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit difficult to interpret the model importances with so many features.  Let's try condesning the features by average feature per team and see if that returns more interpretable importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Condensed Features\n",
    "\n",
    "We'll create a function to condense the features on each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:00.469005Z",
     "start_time": "2021-07-26T16:19:00.349974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert model dataframe from individual players stats to averaged team stats\n",
    "def condense_features(df, feature_list):\n",
    "    \n",
    "    # Empty dataframe\n",
    "    df_total=pd.DataFrame()\n",
    "    \n",
    "    # Copy PlayerWin column\n",
    "    df_total['PlayerWin'] = df['PlayerWin']\n",
    "    \n",
    "    # Loop through features for each player\n",
    "    for feature in feature_list:\n",
    "        \n",
    "        # Add sum features for respective teams\n",
    "        df_total[f'Player_{feature}'] = df[f'P1_{feature}'] + df[f'P2_{feature}'] + df[f'P3_{feature}'] + df[f'P4_{feature}']\n",
    "        df_total[f'Enemy_{feature}'] = df[f'E1_{feature}'] + df[f'E2_{feature}'] + df[f'E3_{feature}'] + df[f'E4_{feature}']\n",
    "        \n",
    "        # Divide by 4 to get average\n",
    "        df_total[f'Player_{feature}'] = df_total[f'Player_{feature}'] / 4\n",
    "        df_total[f'Enemy_{feature}'] = df_total[f'Enemy_{feature}'] / 4\n",
    "\n",
    "    return df_total\n",
    "    \n",
    "df = condense_features(df, feature_list)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:00.472005Z",
     "start_time": "2021-07-26T16:19:00.470003Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_csv('data/Model_W_Condensed_Features_HuskyRaid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:00.539017Z",
     "start_time": "2021-07-26T16:19:00.473003Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Model_W_Condensed_Features_HuskyRaid.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll apply our train test split again.  Before checking out the features, let's see if condensing the features has any affect on model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:00.553021Z",
     "start_time": "2021-07-26T16:19:00.540018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make model_df from a copy of our dataframe up to this point\n",
    "model_df = df.copy()\n",
    "\n",
    "# Assign features and target\n",
    "features = model_df.drop(['PlayerWin'], axis=1)\n",
    "target = model_df['PlayerWin']\n",
    "\n",
    "# Assigning X and y for train test split\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# Ensure target is integer format\n",
    "y = y.astype('int')\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=8)\n",
    "\n",
    "# Print shape\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression, Random Forest, SVM, XGBoost\n",
    "\n",
    "We'll rerun each model we tested before, but now with our condensed feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:36.893731Z",
     "start_time": "2021-07-26T16:19:00.554021Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression with PowerTransformer')\n",
    "make_model(model_df, scale=True, graphs=False, regressor=LogisticRegression(), scaler=PowerTransformer())\n",
    "print('Logistic Regression with StandardScaler')\n",
    "make_model(model_df, scale=True, graphs=False, regressor=LogisticRegression(), scaler=StandardScaler())\n",
    "print('Logistic Regression with RobustScaler')\n",
    "make_model(model_df, scale=True, graphs=False, regressor=LogisticRegression(), scaler=RobustScaler())\n",
    "\n",
    "# Random forest with power transformer scale\n",
    "print('Random Forest with PowerTransformer')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=RandomForestClassifier(random_state=8), scaler=PowerTransformer())\n",
    "print('Random Forest with StandardScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=RandomForestClassifier(random_state=8), scaler=StandardScaler())\n",
    "print('Random Forest with RobustScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=RandomForestClassifier(random_state=8), scaler=RobustScaler())\n",
    "\n",
    "# Support Vector Machines with different scalers\n",
    "print('Support Vector Machine with PowerTransformer')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=svm.SVC(), scaler=PowerTransformer())\n",
    "print('Support Vector Machine with StandardScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=svm.SVC(), scaler=StandardScaler())\n",
    "print('Support Vector Machine with RobustScaler')\n",
    "make_model(model_df, scale=True, graphs=False, \n",
    "           regressor=svm.SVC(), scaler=RobustScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we beat our highest score with the condensed feature set!  Let's take a look at logistic regression with the power transformer in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:37.989978Z",
     "start_time": "2021-07-26T16:19:36.894732Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression with PowerTransformer')\n",
    "make_model(model_df, scale=True, graphs=True, regressor=LogisticRegression(), scaler=PowerTransformer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensed Feature Analysis\n",
    "\n",
    "Let's take a look at the feature importances now that we have condensed the featuers.  We'll perform our train / test split again using the power transformer, and then we'll run our feature importances function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:38.385066Z",
     "start_time": "2021-07-26T16:19:37.990978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make model_df from a copy of our dataframe up to this point\n",
    "model_df = df.copy()\n",
    "\n",
    "# Assign features and target\n",
    "features = model_df.drop(['PlayerWin'], axis=1)\n",
    "target = model_df['PlayerWin']\n",
    "\n",
    "# Assigning X and y for train test split\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# Ensure target is integer format\n",
    "y = y.astype('int')\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=8)\n",
    "\n",
    "# Define and scale X train and test\n",
    "scaler = PowerTransformer()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "# Create and fit logistic regressoin\n",
    "model_log = LogisticRegression()\n",
    "model_log.fit(X_train, y_train)\n",
    "\n",
    "# Print shape\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:38.869176Z",
     "start_time": "2021-07-26T16:19:38.386067Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_coefficients(model_log, features, X_train, X_test, y_train, y_test, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the slimmed down features, we see that Player Winrate is now the most important feature as well as player shots landed per game.  It's still a bit difficult to understand why something like ShotsLandedPerGame is ranked so high, but perhaps this speaks to accuracy being an important factor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Removing Multicollinearity\n",
    "\n",
    "We'll take one last step regarding model interpretation by removing multicollinearity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:38.933190Z",
     "start_time": "2021-07-26T16:19:38.870176Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Model_W_Condensed_Features_HuskyRaid.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "model_df = df.copy()\n",
    "\n",
    "columns = ['PlayerWin', 'Player_WinRate', 'Enemy_WinRate', 'Player_K/D',\n",
    "       'Enemy_K/D', 'Player_Accuracy', 'Enemy_Accuracy',\n",
    "       'Player_TotalGamesCompleted', 'Enemy_TotalGamesCompleted',\n",
    "       'Player_SpartanRank', 'Enemy_SpartanRank',\n",
    "       'Player_KillsPerGame', 'Enemy_KillsPerGame',\n",
    "       'Player_HeadshotsPerGame', 'Enemy_HeadshotsPerGame',\n",
    "       'Player_DeathsPerGame','Enemy_DeathsPerGame', \n",
    "       'Player_GrenadeKillsPerGame', 'Enemy_GrenadeKillsPerGame']\n",
    "\n",
    "model_df = model_df[columns]\n",
    "\n",
    "model_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:40.994655Z",
     "start_time": "2021-07-26T16:19:38.934191Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Create function to output multicollinearity heatmap\n",
    "def heatmap(df_name, figsize=(30,30), cmap='Reds'):\n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        corr = df_name.drop('PlayerWin',axis=1).corr()\n",
    "        mask = np.zeros_like(corr)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(corr, annot=True, cmap=cmap, mask=mask)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "heatmap(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we were able to get rid of multicollinear features that broached the 0.75 threshold.  Let's run the train test split again and check out the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:41.128685Z",
     "start_time": "2021-07-26T16:19:40.995655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign features and target\n",
    "features = model_df.drop(['PlayerWin'], axis=1)\n",
    "target = model_df['PlayerWin']\n",
    "\n",
    "# Assigning X and y for train test split\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# Ensure target is integer format\n",
    "y = y.astype('int')\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=8)\n",
    "\n",
    "# Define and scale X train and test\n",
    "scaler = PowerTransformer()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "# Create and fit logistic regressoin\n",
    "model_log = LogisticRegression()\n",
    "model_log.fit(X_train, y_train)\n",
    "\n",
    "# Print shape\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:41.628798Z",
     "start_time": "2021-07-26T16:19:41.129685Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_coefficients(model_log, features, X_train, X_test, y_train, y_test, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Player_WinRate is still the most important feature in predicting the outcome of a match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model\n",
    "\n",
    "We explored many model types with a variety of scalers and datasets.  The best model for predicting victory using only gamertags and data available from the API was the per-game statistics condensed by player and enemy teams using logistic regression and the power transformer for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T16:19:42.845073Z",
     "start_time": "2021-07-26T16:19:41.629799Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "# sns.set(rc={'axes.facecolor':'white'})\n",
    "\n",
    "df = pd.read_csv('data/Model_W_Condensed_Features_HuskyRaid.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# Make model_df from a copy of our dataframe up to this point\n",
    "model_df = df.copy()\n",
    "\n",
    "# Assign features and target\n",
    "features = model_df.drop(['PlayerWin'], axis=1)\n",
    "target = model_df['PlayerWin']\n",
    "\n",
    "# Assigning X and y for train test split\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# Ensure target is integer format\n",
    "y = y.astype('int')\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=8)\n",
    "\n",
    "# Print shape\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print('Logistic Regression with PowerTransformer')\n",
    "make_model(model_df, scale=True, graphs=True, regressor=LogisticRegression(), scaler=PowerTransformer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is more precise at predicting victory over defeat, and this might be due to the fact that the matches compiled for the dataframe came from players that were fairly experienced in the Super Fiesta Party playlist.  In the future, it might be worth exploring matches from competitors with less play time and less experience in the playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Recommendations\n",
    "\n",
    "For this specific playlist and with players who were most likely more skilled than average, we were able to predict the victor of a match with 72.73% accuracy using only information gathered by the API and no details about what actually occurred during the match.  \n",
    "\n",
    "While I admittedly have no knowledge on best practices in ensuring a positive player experience, I believe an ideal matchmaking algorithm should not be predictable above a certain threshold, ideally not much higher than 50%.\n",
    "\n",
    "It is entirely possible that matchmaking algorithms are already optimized to meet this ideal standard.  Perhaps sourcing modeling data from more skilled players in a very specific playlist would naturally lead to a higher than desired predictive quality simply because there are not enough equally skilled players entering matchmaking to ensure an even match at various hours of the day.\n",
    "\n",
    "However, if that's not the case, a solution to uneven matchmaking might come in the form of a machine learning model as simple and efficient as logistic regression using readily available player data.  If something like this isn't being used, it could be implemented experimentally.\n",
    "\n",
    "I should note that none of the modeling was conducted with ranked matchmaking, which certainly exists in Halo 5 and many other competitive games.  That system is likely more nuanced and robust, and deserves its own round of modeling and analysis.\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "Regarding the Super Fiesta Party playlist, where players spawn with random weapons throughout the match, there exists a 'Match Events' API call that details nearly every action that happened in any given match.  Most importantly, this provides information on what weapons players spawned with throughout the match.  Given the fact that the weapons are randomized, frequenters of Super Fiesta Party will (or should) freely admit that luck with the random weapons varies substantially.\n",
    "\n",
    "This project was originally concieved with this in mind, and the goal was to predict victory based on random weapon spawns alone.  The hurdle we encountered was that there was not a way to decode the +100 weapon variants.  343 Industries admitted in a forum post that adding this to the API would not be trivial, and given the API is technically a beta, they're under no obligation to give us this information.  However, it should be possible to decode the weapon variants through some individual data collection conducted through custom matches.\n",
    "\n",
    "Finally, we would like to exapnd our modeling dataset to a variety of skill levels and playlists, which will be possible by identifying players that meet this criteria.  It would certainly be worthwhile to determine whether or not ranked matchmaking has the same level of predictive quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235.819px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
